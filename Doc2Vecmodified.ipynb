{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"Cafawu6rmDay","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1680066240290,"user_tz":-330,"elapsed":24548,"user":{"displayName":"navadeep chowdary","userId":"17512258322492939931"}},"outputId":"8984fe69-5b2b-4e9d-fa8d-6f4342fd140f"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["from gensim.test.utils import common_texts\n","from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n","from sklearn.metrics import accuracy_score, f1_score\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn import utils\n","import csv\n","from tqdm import tqdm\n","import multiprocessing\n","import nltk\n","from nltk.corpus import stopwords "],"metadata":{"id":"FERmiPQxmkn_","executionInfo":{"status":"ok","timestamp":1680066247379,"user_tz":-330,"elapsed":7092,"user":{"displayName":"navadeep chowdary","userId":"17512258322492939931"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["nltk.download('punkt')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LACIuhUPnSQQ","executionInfo":{"status":"ok","timestamp":1680066248167,"user_tz":-330,"elapsed":801,"user":{"displayName":"navadeep chowdary","userId":"17512258322492939931"}},"outputId":"bf5c2247-5cfc-4979-c7dc-2b411d771a1e"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":3}]},{"cell_type":"code","source":["#Reading and preparing the text\n","tqdm.pandas(desc=\"progress-bar\")\n","# Function for tokenizing\n","def tokenize_text(text):\n","    tokens = []\n","    for sent in nltk.sent_tokenize(text):\n","        for word in nltk.word_tokenize(sent):\n","            if len(word) < 2:\n","                continue\n","            tokens.append(word.lower())\n","    return tokens\n","# Initializing the variables\n","train_documents = []\n","test_documents = []\n","i = 0\n","# Associating the tags(labels) with numbers\n","tags_index = {'sci-fi': 1 , 'action': 2, 'comedy': 3, 'fantasy': 4, 'animation': 5, 'romance': 6}\n","#Reading the file\n","FILEPATH = '/content/drive/MyDrive/tagged_plots_movielens.csv'\n","with open(FILEPATH, 'r') as csvfile:\n","    with open('/content/drive/MyDrive/tagged_plots_movielens.csv', 'r') as csvfile:\n","        moviereader = csv.reader(csvfile, delimiter=',', quotechar='\"')\n","        for row in moviereader:\n","            if i == 0:\n","                i += 1\n","                continue\n","            i += 1\n","            if i <= 2000:\n","                train_documents.append(TaggedDocument(words=tokenize_text(row[2]), tags=[tags_index.get(row[3], 8)] ))\n","            else:\n","                test_documents.append( TaggedDocument(words=tokenize_text(row[2]),\n","tags=[tags_index.get(row[3], 8)]))\n","print(train_documents[0])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":235},"id":"x3j9xSTBmv8N","executionInfo":{"status":"error","timestamp":1680066256708,"user_tz":-330,"elapsed":469,"user":{"displayName":"navadeep chowdary","userId":"17512258322492939931"}},"outputId":"508cd215-e3c4-4b96-c323-8cce444f9e8d"},"execution_count":5,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-ab78bbe317b8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m#Reading the file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mFILEPATH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/MyDrive/tagged_plots_movielens.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFILEPATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcsvfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/MyDrive/tagged_plots_movielens.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcsvfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mmoviereader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcsvfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquotechar\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\"'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/tagged_plots_movielens.csv'"]}]},{"cell_type":"code","source":["#getting feature vector from Doc2Vec model\n","cores = multiprocessing.cpu_count()\n","\n","model_dbow = Doc2Vec(dm=1, vector_size=300, negative=5, hs=0, min_count=2, sample = 0, workers=cores, alpha=0.025, min_alpha=0.001)\n","model_dbow.build_vocab([x for x in tqdm(train_documents)])\n","train_documents  = utils.shuffle(train_documents)\n","model_dbow.train(train_documents,total_examples=len(train_documents), epochs=30)\n","def vector_for_learning(model, input_docs):\n","    sents = input_docs\n","    targets, feature_vectors = zip(*[(doc.tags[0], model.infer_vector(doc.words, steps=20)) for doc in sents])\n","    return targets, feature_vectors\n","model_dbow.save('./movieModel.d2v')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tHmiRI19nAEI","executionInfo":{"status":"ok","timestamp":1677329345207,"user_tz":-330,"elapsed":21551,"user":{"displayName":"dracula king","userId":"08736303197257505361"}},"outputId":"1b9bb451-9940-4d5c-b348-3c8d104229ed"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 1999/1999 [00:00<00:00, 1388147.96it/s]\n"]}]},{"cell_type":"code","source":["#Training the classifier\n","y_train, X_train = vector_for_learning(model_dbow, train_documents)\n","y_test, X_test = vector_for_learning(model_dbow, test_documents)\n","\n","logreg = LogisticRegression(n_jobs=1, C=1e5)\n","logreg.fit(X_train, y_train)\n","y_pred = logreg.predict(X_test)\n","print('Testing accuracy for movie plots%s' % accuracy_score(y_test, y_pred))\n","print('Testing F1 score for movie plots: {}'.format(f1_score(y_test, y_pred, average='weighted')))\n","#you can calculate the precision recall by yourself"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JgvYv73knk_Z","executionInfo":{"status":"ok","timestamp":1677329399443,"user_tz":-330,"elapsed":19898,"user":{"displayName":"dracula king","userId":"08736303197257505361"}},"outputId":"b489a1aa-b0df-4efb-838d-e65eb7201f15"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Testing accuracy for movie plots0.4209354120267261\n","Testing F1 score for movie plots: 0.41362023489671457\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.8/dist-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","Please also refer to the documentation for alternative solver options:\n","    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n","  n_iter_i = _check_optimize_result(\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"kJY_Aar5ny1o"},"execution_count":null,"outputs":[]}]}